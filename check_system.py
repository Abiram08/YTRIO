#!/usr/bin/env python3
"""
System information checker for EchoVerse troubleshooting
"""

import os
import sys
import platform
import psutil
import torch
import subprocess

def check_system():
    """Check system requirements and provide diagnostics"""
    
    print("ğŸ” EchoVerse System Diagnostic")
    print("=" * 50)
    
    # Python version
    print(f"ğŸ Python Version: {sys.version}")
    
    # Operating System
    print(f"ğŸ’» OS: {platform.system()} {platform.version()}")
    print(f"ğŸ“± Architecture: {platform.machine()}")
    
    # Memory Information
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ Total RAM: {memory.total / (1024**3):.1f} GB")
    print(f"ğŸ’¾ Available RAM: {memory.available / (1024**3):.1f} GB")
    print(f"ğŸ’¾ Memory Usage: {memory.percent}%")
    
    # Disk Space
    try:
        disk = psutil.disk_usage('.')
        print(f"ğŸ’½ Disk Space (current dir): {disk.free / (1024**3):.1f} GB free of {disk.total / (1024**3):.1f} GB")
    except:
        print("ğŸ’½ Disk Space: Could not determine")
    
    # PyTorch Information
    print(f"ğŸ”¥ PyTorch Version: {torch.__version__}")
    print(f"ğŸ”¥ CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ”¥ CUDA Version: {torch.version.cuda}")
        print(f"ğŸ”¥ GPU Device: {torch.cuda.get_device_name()}")
    
    # Check model directory
    model_path = "./models/granite-2b"
    if os.path.exists(model_path):
        print(f"ğŸ“ Model Directory: Found at {model_path}")
        files = os.listdir(model_path)
        print(f"ğŸ“ Model Files: {len(files)} files")
        
        # Check for key files
        key_files = ['config.json', 'tokenizer.json', 'model.safetensors.index.json']
        for file in key_files:
            if file in files:
                print(f"  âœ… {file}")
            else:
                print(f"  âŒ {file} (missing)")
        
        # Check model file sizes
        safetensor_files = [f for f in files if f.endswith('.safetensors')]
        total_size = 0
        for file in safetensor_files:
            try:
                size = os.path.getsize(os.path.join(model_path, file))
                total_size += size
                print(f"  ğŸ“¦ {file}: {size / (1024**3):.2f} GB")
            except:
                print(f"  ğŸ“¦ {file}: Size unknown")
        
        print(f"ğŸ“¦ Total Model Size: {total_size / (1024**3):.2f} GB")
    else:
        print(f"ğŸ“ Model Directory: Not found at {model_path}")
    
    # Recommendations
    print("\nğŸ’¡ Recommendations:")
    
    if memory.available / (1024**3) < 8:
        print("  âš ï¸ Low available RAM. Close other applications before running EchoVerse")
    else:
        print("  âœ… Sufficient RAM available")
    
    if os.path.exists(model_path):
        if total_size / (1024**3) < 2:
            print("  âš ï¸ Model files seem incomplete. Consider re-downloading")
        else:
            print("  âœ… Model files appear complete")
    else:
        print("  âŒ Model directory not found. Model will be downloaded from Hugging Face")
    
    # Test basic imports
    print("\nğŸ§ª Testing imports...")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        print("  âœ… Transformers library working")
    except Exception as e:
        print(f"  âŒ Transformers import failed: {e}")
    
    try:
        import streamlit
        print("  âœ… Streamlit library working")
    except Exception as e:
        print(f"  âŒ Streamlit import failed: {e}")

if __name__ == "__main__":
    try:
        check_system()
    except Exception as e:
        print(f"âŒ System check failed: {e}")
        import traceback
        traceback.print_exc()